# Temporal-Difference-Method
Replication of results found in figure 3,4 and 5 from Richard Sutton’s paper *Learning to Predict by the Methods of Temporal Differences*. This is the course project of CS7642 Reinforcement Learning.

Course Instructor has all the rights on course materials, homeworks, exams and projects. Please note that unauthorized use of any previous semester course materials, such as tests, quizzes, homework, projects, videos, and any other coursework, is prohibited in this course.


## 1. Bounded Random Walk

A bounded random walk is a state sequence generated by taking random steps to the left or right unti a termination state is reached. In this experiment, each walk starts from state D. At each step, the walk moves to either the left or the right nebouring state with equal probability. Once the termination state (A or G) is reached, the walk terminates. We define z = 0 for a random walk ends in state A and 1 for a walk ends in state G. The learner will estimate the expected value of z, which is equal to the probability of ending in state G. 

We use Root Mean Squared Error (RMSE) to measure the performance of the model on each training set and then average the RMSE over all training sets. In the context of this project, it is to compare for each component of the weight vector the predicted values and true probabilities. In this project, 1000 random walk sequences are generated and are split into 100 training sets.

## 2. Experiment on λ

Figure 3 aims to compare the performance of different λ with same α. The weight vector is updated after the presence of all 10 sequences in a training set. In addition, each training set feeds repeatedly until the weight vector converges: the maximum of each component of the weight vector is less than a predetermined constant epsilon(10e-6). The average RMSE over 100 training sets is the error of this λ.

The results share the same trend globally (errors increase) though the errors for all λ are lower. There are multiple reasons. It may because the model overfits the training set as a result of feeding the training sets repeatedly,the way of storing values evoluted and also a different seed. 

## 3. Experiment on learning rate

Figure 4 focuses on the performance of learning procedures under different α values. In this experiment, each learning procedure are tested against α ranges from 0 to 0.6 with increment of 0.05. λ ranges between in 0 and 1 with increment of 0.1. In figure 4 of Sutton’s paper, only λ = 0, 0.3, 0.8, 1 are plotted. 

* Weight vector is updated after each sequence;
* Unlike first experiment, a single training set is not presented for multiple times instead we only feed once each training set; 
* The initial values are set to 0.5 for all the components in order not to bias any of the states; 
* Two versions of random walk are tested, one with limit of sequence length (discards the sequence length longer than 14) and one without limit.

With no limit on sequence length, the error of TD(0) grows rapidly when α is greater than 0.5 and even exceeds error of TD(1). This is different from Figure 4 in Sutton’s paper where TD(1) has the highest error for all α values. However, when sequence length limit is introduced, the patterns matches Sutton’s figure much better: TD(1) has the highest error for almost all α values and the best errors were achieved with intermediate α values.

The reason why result of the random walk with no limit on sequence length differs from Figure 4 from Sutton’s paper may be that when learning rate increases, TD(0) tends to overfit more easily and causes a higher variance. This could also be supported by the fact that when different seed are tested, error of TD(0) has a quite wide range. Also, as each training set is presented only once and TD(0) is relatively slow at learning, the procedure is far from convergence. Therefore, the introduction of limit on sequence length can reduce some randomness and the possibility of overfitting. In the meantime, it might also be more realistic that the random walk will not goes back and forth so many times before it reaches the termination state considering there are only 7 states in total.

## 4.Best error for each λ

Figure 5 in Sutton’s paper summarised best error level achieved for each λ using the best α value for that λ. 
